{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e078f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sentence_add_manually(dataset):\n",
    "    columns = ['문장','여성/가족','남성','성소수자','인종/국적','연령','지역','종교','기타 혐오','악플/욕설','clean','개인지칭']\n",
    "    new_sentence = []\n",
    "    \n",
    "    for col in columns:\n",
    "        if col =='문장':\n",
    "            new_sentence.append(input(f'{col} :'))\n",
    "        else:\n",
    "            new_sentence.append(int(input(f'{col} :'))) \n",
    "    \n",
    "    sentence = pd.DataFrame(columns=columns)\n",
    "    sentence.loc[0] = new_sentence\n",
    "    return pd.concat([dataset,sentence],ignore_index=True)\n",
    "\n",
    "\n",
    "def sentence_add_auto(dataset,new_sentence):\n",
    "    columns = ['문장','여성/가족','남성','성소수자','인종/국적','연령','지역','종교','기타 혐오','악플/욕설','clean','개인지칭']\n",
    "    sentence = pd.DataFrame(columns=columns)\n",
    "    sentence.loc[0] = new_sentence\n",
    "    return pd.concat([dataset,sentence],ignore_index=True)\n",
    "\n",
    "\n",
    "def dataset_add_auto(dataset,new_dataset):\n",
    "    return pd.concat([dataset,new_dataset],ignore_index=True)\n",
    "\n",
    "\n",
    "# sentence_add_auto(valid.head(5),[\"한남충 씨발아\",0,1,0,0,0,0,0,0,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175f75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "0it [00:00, ?it/s]<ipython-input-2-b35589aa0356>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.tokenized_sentence.items()}\n",
      "1876it [01:32, 20.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertModel,BertForSequenceClassification,AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "class PseudoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_sentence):\n",
    "        self.tokenized_sentence = tokenized_sentence\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx]) for key, value in self.tokenized_sentence.items()}\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(tokenized_psuedo[\"input_ids\"])\n",
    "    \n",
    "\n",
    "\n",
    "def pseudo_data_eval(pseudo_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pseudo_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(pseudo_loader)):\n",
    "            \n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            pseudo_outputs.extend(torch.sigmoid(outputs[\"logits\"]).cpu().detach().numpy().tolist())\n",
    "            \n",
    "    \n",
    "    return pseudo_outputs\n",
    "\n",
    "def pseudo_labeled_data_generator(pseudo_outputs,dataset,k=0.997):\n",
    "    \n",
    "    pseudo_outputs = np.asarray(pseudo_outputs)\n",
    "    boolean_index = (pseudo_outputs).max(axis=1) > k\n",
    "    \n",
    "    labels = pseudo_outputs[boolean_index].argmax(axis=1)\n",
    "    \n",
    "    pseudo_dataset = pd.DataFrame(columns=['문장','여성/가족','남성','성소수자','인종/국적','연령','지역','종교','기타 혐오','악플/욕설','clean','개인지칭'])\n",
    "    pseudo_dataset[\"문장\"] = dataset[boolean_index]\n",
    "    pseudo_dataset['clean'] = labels\n",
    "    pseudo_dataset.fillna(0,inplace=True)\n",
    "    \n",
    "    return pseudo_dataset\n",
    "\n",
    "# def psuedo_labeled_data_add(psuedo_outputs,dataset,k=0.997):\n",
    "    \n",
    "#     psuedo_outputs = np.asarray(psuedo_outputs)\n",
    "#     boolean_index = (psuedo_outputs).max(axis=1) > k\n",
    "\n",
    "#     labels = psuedo_outputs[boolean_index].argmax(axis=1)\n",
    "\n",
    "#     psuedo_dataset = dataset[boolean_index]\n",
    "#     psuedo_dataset.loc[:,'clean'] = labels\n",
    "    \n",
    "#     return psuedo_dataset\n",
    "\n",
    "\n",
    "\n",
    "# original_dataset: 원래 있던 dataset(train dataset), psuedo_sentences: unlabeled된 sentences\n",
    "def pseudo_labeling(pseudo_sentences,original_dataset=None):\n",
    "    \n",
    "    unlabeled_dataset = PseudoDataset(tokenized_psuedo)\n",
    "    pseudo_params = {'batch_size': 8,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    pseudo_loader = DataLoader(unlabeled_dataset, **pseudo_params)\n",
    "    pseudo_outputs = pseudo_data_eval(pseudo_loader)\n",
    "    pseudo_labeled_dataset = pseudo_labeled_data_generator(pseudo_outputs,pseudo_sentences)\n",
    "    \n",
    "    return dataset_add_auto(original_dataset,pseudo_labeled_dataset)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#이 부분만 데이터 바꿔 주면 된다\n",
    "# pseudo_sentences => unlabeled된 sentences들(Series형태)\n",
    "pseudo = pd.read_csv(\"./Dataset/unsmile_train_v1.0.tsv\", sep=\"\\t\")\n",
    "pseudo_sentences = pseudo[\"문장\"]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "PATH = \"./model_fp/Roberta3.pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenized_psuedo = tokenizer(\n",
    "                    list(pseudo_sentences), \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=256, \n",
    "                    add_special_tokens=True,\n",
    "                )\n",
    "\n",
    "\n",
    "new_data = pseudo_labeling(pseudo_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37458c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16332\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5110\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5110' max='5110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5110/5110 53:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output9/checkpoint-1000\n",
      "Configuration saved in ./output9/checkpoint-1000/config.json\n",
      "Model weights saved in ./output9/checkpoint-1000/pytorch_model.bin\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n",
      "Saving model checkpoint to ./output9/checkpoint-2000\n",
      "Configuration saved in ./output9/checkpoint-2000/config.json\n",
      "Model weights saved in ./output9/checkpoint-2000/pytorch_model.bin\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n",
      "Saving model checkpoint to ./output9/checkpoint-3000\n",
      "Configuration saved in ./output9/checkpoint-3000/config.json\n",
      "Model weights saved in ./output9/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [output9/checkpoint-1000] due to args.save_total_limit\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n",
      "Saving model checkpoint to ./output9/checkpoint-4000\n",
      "Configuration saved in ./output9/checkpoint-4000/config.json\n",
      "Model weights saved in ./output9/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [output9/checkpoint-2000] due to args.save_total_limit\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n",
      "Saving model checkpoint to ./output9/checkpoint-5000\n",
      "Configuration saved in ./output9/checkpoint-5000/config.json\n",
      "Model weights saved in ./output9/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [output9/checkpoint-3000] due to args.save_total_limit\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5110, training_loss=0.08146084547042846, metrics={'train_runtime': 3186.231, 'train_samples_per_second': 51.258, 'train_steps_per_second': 1.604, 'total_flos': 3.359168441257104e+16, 'train_loss': 0.08146084547042846, 'epoch': 10.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model을 수도 라벨링한 데이터랑 원래 데이터랑 합쳐서 새로운 데이터셋 만들고 Retrain!!!\n",
    "class myDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoding, label):\n",
    "        self.encoding = encoding\n",
    "        self.label = label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n",
    "        item[\"label\"] = torch.tensor(self.label[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.label\n",
    "    \n",
    "def compute_metrics(preds):\n",
    "    labels = preds.label_ids\n",
    "    preds = preds.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"acc\": acc, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "new_train = dataset_add_auto(pseudo,new_data)\n",
    "\n",
    "tokenized_new_train = tokenizer(\n",
    "                    list(new_train[\"문장\"]), \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=256, \n",
    "                    add_special_tokens=True\n",
    "                )\n",
    "train_new_label = new_train['clean'].values\n",
    "train_new_dataset = myDataset(tokenized_new_train, train_new_label)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output9\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=5e-6,\n",
    "    logging_dir=\"./log\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "#todo\n",
    "LENGTH = int(len(train_new_dataset))\n",
    "train_data, val_data = random_split(train_new_dataset, [int(LENGTH*0.8), LENGTH-int(LENGTH*0.8)])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "             \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ed029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3737\n",
      "  Batch size = 64\n",
      "<ipython-input-3-ac363b8b24a6>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[idx]) for key, value in self.encoding.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9036511182785034,\n",
       " 'eval_acc': 0.8694139684238694,\n",
       " 'eval_precision': 0.7431991294885746,\n",
       " 'eval_recall': 0.7304812834224599,\n",
       " 'eval_f1': 0.7367853290183387,\n",
       " 'eval_runtime': 20.6321,\n",
       " 'eval_samples_per_second': 181.126,\n",
       " 'eval_steps_per_second': 2.86,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./Dataset/unsmile_valid_v1.0.tsv\", sep=\"\\t\")\n",
    "tokenized_test = tokenizer(\n",
    "                    list(test[\"문장\"]), \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=256, \n",
    "                    add_special_tokens=True\n",
    "                )\n",
    "test_label = test['clean'].values\n",
    "testdataset = myDataset(tokenized_test, test_label)\n",
    "trainer.evaluate(eval_dataset=testdataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d29624fa02f72a2f2eb64b5fa4dfbc751609e2b6c88be691c0db207c64cc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
